{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296},{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289},{"sourceId":53008,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":44501,"modelId":61455}],"dockerImageVersionId":30198,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\nfrom tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional\nfrom tensorflow.keras.applications import VGG16, ResNet50, DenseNet201\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom textwrap import wrap\nfrom nltk import pos_tag\nfrom wordcloud import WordCloud\nfrom nltk.tokenize import word_tokenize\n\n\nplt.rcParams['font.size'] = 12\nsns.set_style(\"dark\")\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-21T11:05:34.128899Z","iopub.execute_input":"2024-05-21T11:05:34.129381Z","iopub.status.idle":"2024-05-21T11:05:40.472986Z","shell.execute_reply.started":"2024-05-21T11:05:34.129266Z","shell.execute_reply":"2024-05-21T11:05:40.472086Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Image Captioning**\n\n**What is Image Captioning ?**\n- Image Captioning is the process of generating textual description of an image. It uses both Natural Language Processing and Computer Vision to generate the captions.\n- This task lies at the intersection of computer vision and natural language processing. Most image captioning systems use an encoder-decoder framework, where an input image is encoded into an intermediate representation of the information in the image, and then decoded into a descriptive text sequence.\n\n**CNNs + RNNs (LSTMs)**\n- To perform Image Captioning we will require two deep learning models combined into one for the training purpose\n- CNNs extract the features from the image of some vector size aka the vector embeddings. The size of these embeddings depend on the type of pretrained network being used for the feature extraction\n- LSTMs are used for the text generation process. The image embeddings are concatenated with the word embeddings and passed to the LSTM to generate the next word\n- For a more illustrative explanation of this architecture check the Modelling section for a picture representation","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/max/1400/1*6BFOIdSHlk24Z3DFEakvnQ.png\">","metadata":{}},{"cell_type":"code","source":"image_path = '../input/flickr8k/Images'","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:05:42.239619Z","iopub.execute_input":"2024-05-21T11:05:42.240747Z","iopub.status.idle":"2024-05-21T11:05:42.244982Z","shell.execute_reply.started":"2024-05-21T11:05:42.240701Z","shell.execute_reply":"2024-05-21T11:05:42.244076Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/flickr8k/captions.txt\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:05:44.508353Z","iopub.execute_input":"2024-05-21T11:05:44.509266Z","iopub.status.idle":"2024-05-21T11:05:44.610911Z","shell.execute_reply.started":"2024-05-21T11:05:44.509222Z","shell.execute_reply":"2024-05-21T11:05:44.610110Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                       image  \\\n0  1000268201_693b08cb0e.jpg   \n1  1000268201_693b08cb0e.jpg   \n2  1000268201_693b08cb0e.jpg   \n3  1000268201_693b08cb0e.jpg   \n4  1000268201_693b08cb0e.jpg   \n\n                                             caption  \n0  A child in a pink dress is climbing up a set o...  \n1              A girl going into a wooden building .  \n2   A little girl climbing into a wooden playhouse .  \n3  A little girl climbing the stairs to her playh...  \n4  A little girl in a pink dress going into a woo...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A child in a pink dress is climbing up a set o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A girl going into a wooden building .</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing into a wooden playhouse .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing the stairs to her playh...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl in a pink dress going into a woo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def readImage(path,img_size=224):\n    img = load_img(path,color_mode='rgb',target_size=(img_size,img_size))\n    img = img_to_array(img)\n    img = img/255.\n    \n    return img\n\ndef display_images(temp_df):\n    temp_df = temp_df.reset_index(drop=True)\n    plt.figure(figsize = (20 , 20))\n    n = 0\n    for i in range(15):\n        n+=1\n        plt.subplot(5 , 5, n)\n        plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n        image = readImage(f\"../input/flickr8k/Images/{temp_df.image[i]}\")\n        plt.imshow(image)\n        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n        plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:05:47.107788Z","iopub.execute_input":"2024-05-21T11:05:47.108686Z","iopub.status.idle":"2024-05-21T11:05:47.116370Z","shell.execute_reply.started":"2024-05-21T11:05:47.108647Z","shell.execute_reply":"2024-05-21T11:05:47.115461Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **Visualization**\n- Images and their corresponding captions","metadata":{}},{"cell_type":"code","source":"display_images(data.sample(15))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"font-weight: bold; color: #007bff;\">Distribution of Comments per Image:</span>\n\nVisualize the distribution of comments per image to understand how many comments are typically associated with each image.","metadata":{}},{"cell_type":"code","source":"comments_per_image = data.groupby('image')['caption'].count()\n\nimages_with_4_comments = comments_per_image[comments_per_image == 4]\nimages_with_5_comments = comments_per_image[comments_per_image == 5]\n\n# Plot the bar chart\nplt.figure(figsize=(10, 5))\nplt.bar([3.5, 4.5], [len(images_with_4_comments), len(images_with_5_comments)], tick_label=['4 Comments', '5 Comments'])\nplt.xlabel('Number of Comments')\nplt.ylabel('Number of Images')\nplt.title('Distribution of Comments per Image')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"font-weight: bold; color: #007bff;\">Word Cloud:</span>\n\nGenerate a word cloud to visualize the most frequent words in the comments.","metadata":{}},{"cell_type":"code","source":"\nall_comments_text = ' '.join(data['caption'].fillna(''))\n\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_comments_text)\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud of Comments')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['pos_tags'] = data['caption'].fillna(\"\").apply(lambda x: pos_tag(word_tokenize(x.lower())))\n\nnouns = [word[0] for tags in data['pos_tags'] for word in tags if word[1].startswith('NN')]\n\nwordcloud_nouns = WordCloud(width=800, height=400, background_color='white').generate(' '.join(nouns))\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud_nouns, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud of Nouns in Comments')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <span style=\"font-weight: bold; color: #007bff;\">Caption Length Distribution:</span>\n\nAnalyze the distribution of caption lengths to understand the range of caption lengths in the dataset.","metadata":{}},{"cell_type":"code","source":"# Caption length distribution\ndata['caption_length'] = data['caption'].fillna('').apply(lambda x: len(x.split()))\nplt.hist(data['caption_length'], bins=range(1, max(data['caption_length']) + 1))\nplt.xlabel('Caption Length (in words)')\nplt.ylabel('Number of Captions')\nplt.title('Distribution of Caption Lengths')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Caption Text Preprocessing Steps**\n- Convert sentences into lowercase\n- Remove special characters and numbers present in the text\n- Remove extra spaces\n- Remove single characters\n- Add a starting and an ending tag to the sentences to indicate the beginning and the ending of a sentence","metadata":{}},{"cell_type":"code","source":"def text_preprocessing(data):\n    data['caption'] = data['caption'].apply(lambda x: x.lower())\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\",\" \"))\n    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n    data['caption'] = \"startseq \"+data['caption']+\" endseq\"\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:05:54.180123Z","iopub.execute_input":"2024-05-21T11:05:54.181048Z","iopub.status.idle":"2024-05-21T11:05:54.189236Z","shell.execute_reply.started":"2024-05-21T11:05:54.181004Z","shell.execute_reply":"2024-05-21T11:05:54.188256Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## __Preprocessed Text__","metadata":{}},{"cell_type":"code","source":"data = text_preprocessing(data)\ncaptions = data['caption'].tolist()\ncaptions[:10]","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:05:56.999147Z","iopub.execute_input":"2024-05-21T11:05:56.999607Z","iopub.status.idle":"2024-05-21T11:05:57.203630Z","shell.execute_reply.started":"2024-05-21T11:05:56.999561Z","shell.execute_reply":"2024-05-21T11:05:57.202811Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n 'startseq girl going into wooden building endseq',\n 'startseq little girl climbing into wooden playhouse endseq',\n 'startseq little girl climbing the stairs to her playhouse endseq',\n 'startseq little girl in pink dress going into wooden cabin endseq',\n 'startseq black dog and spotted dog are fighting endseq',\n 'startseq black dog and tri-colored dog playing with each other on the road endseq',\n 'startseq black dog and white dog with brown spots are staring at each other in the street endseq',\n 'startseq two dogs of different breeds looking at each other on the road endseq',\n 'startseq two dogs on pavement moving toward each other endseq']"},"metadata":{}}]},{"cell_type":"markdown","source":"## __Tokenization and Encoded Representation__\n- The words in a sentence are separated/tokenized and encoded in a one hot representation\n- These encodings are then passed to the embeddings layer to generate word embeddings\n\n<img src='https://lena-voita.github.io/resources/lectures/word_emb/lookup_table.gif'>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pickle\n\n# Assuming data is your DataFrame containing captions\ncaptions = data['caption'].tolist()\n\n# Split captions into words and build the vocabulary\nwords = [caption.split() for caption in captions]\nunique = list(set(word for sublist in words for word in sublist))\n\n# Save and load unique words (optional, for future use)\n# with open(\"unique.p\", \"wb\") as pickle_d:\n#     pickle.dump(unique, pickle_d) \n# unique = pickle.load(open('unique.p', 'rb'))\n\n# Create mappings\nword2idx = {val: index for index, val in enumerate(unique)}\nidx2word = {index: val for index, val in enumerate(unique)}\n\n# Example usage of mappings\nprint(word2idx['startseq'])  # Should print the index of 'startseq'\nprint(idx2word[379])        # Should print the word corresponding to index 5553\n\n# Save the mappings to files\nwith open(\"word2idx.pkl\", \"wb\") as f:\n    pickle.dump(word2idx, f)\n\nwith open(\"idx2word.pkl\", \"wb\") as f:\n    pickle.dump(idx2word, f)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:06:01.327778Z","iopub.execute_input":"2024-05-21T11:06:01.328181Z","iopub.status.idle":"2024-05-21T11:06:01.452548Z","shell.execute_reply.started":"2024-05-21T11:06:01.328138Z","shell.execute_reply":"2024-05-21T11:06:01.451656Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"8590\nsmile\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(captions)\nvocab_size = len(tokenizer.word_index) + 1\nmax_length = max(len(caption.split()) for caption in captions)\n\nimages = data['image'].unique().tolist()\nnimages = len(images)\n\nsplit_index = round(0.85*nimages)\ntrain_images = images[:split_index]\nval_images = images[split_index:]\n\ntrain = data[data['image'].isin(train_images)]\ntest = data[data['image'].isin(val_images)]\n\ntrain.reset_index(inplace=True,drop=True)\ntest.reset_index(inplace=True,drop=True)\n\ntokenizer.texts_to_sequences([captions[1]])[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:06:04.227388Z","iopub.execute_input":"2024-05-21T11:06:04.227966Z","iopub.status.idle":"2024-05-21T11:06:05.012529Z","shell.execute_reply.started":"2024-05-21T11:06:04.227909Z","shell.execute_reply":"2024-05-21T11:06:05.011723Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"[1, 18, 315, 63, 195, 116, 2]"},"metadata":{}}]},{"cell_type":"code","source":"with open('tokenizer.pkl', 'wb') as handle:\n    pickle.dump(tokenizer, handle)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:06:06.640703Z","iopub.execute_input":"2024-05-21T11:06:06.641095Z","iopub.status.idle":"2024-05-21T11:06:06.654587Z","shell.execute_reply.started":"2024-05-21T11:06:06.641061Z","shell.execute_reply":"2024-05-21T11:06:06.653701Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# **Image Feature Extraction**\n- DenseNet 201 Architecture is used to extract the features from the images\n- Any other pretrained architecture can also be used for extracting features from these images\n- Since the Global Average Pooling layer is selected as the final layer of the DenseNet201 model for our feature extraction, our image embeddings will be a vector of size 1920\n\n<img src=\"https://imgur.com/wWHWbQt.jpg\">","metadata":{}},{"cell_type":"code","source":"model = DenseNet201(weights='imagenet', include_top=False, pooling='avg')\nfe = Model(inputs=model.input, outputs=model.layers[-2].output)\n\nimg_size = 224\nfeatures = {}\nfor image in tqdm(data['image'].unique().tolist()):\n    img = load_img(os.path.join(image_path,image),target_size=(img_size,img_size))\n    img = img_to_array(img)\n    img = img/255.\n    img = np.expand_dims(img,axis=0)\n    feature = fe.predict(img, verbose=0)\n    features[image] = feature","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Generation**\n- Since Image Caption model training like any other neural network training is a highly resource utillizing process we cannot load the data into the main memory all at once, and hence we need to generate the data in the required format batch wise\n- The inputs will be the image embeddings and their corresonding caption text embeddings for the training process\n- The text embeddings are passed word by word for the caption generation during inference time","metadata":{}},{"cell_type":"code","source":"class CustomDataGenerator(Sequence):\n    \n    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer, \n                 vocab_size, max_length, features,shuffle=True):\n    \n        self.df = df.copy()\n        self.X_col = X_col\n        self.y_col = y_col\n        self.directory = directory\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.features = features\n        self.shuffle = shuffle\n        self.n = len(self.df)\n        \n    def on_epoch_end(self):\n        if self.shuffle:\n            self.df = self.df.sample(frac=1).reset_index(drop=True)\n    \n    def __len__(self):\n        return self.n // self.batch_size\n    \n    def __getitem__(self,index):\n    \n        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size,:]\n        X1, X2, y = self.__get_data(batch)        \n        return (X1, X2), y\n    \n    def __get_data(self,batch):\n        \n        X1, X2, y = list(), list(), list()\n        \n        images = batch[self.X_col].tolist()\n           \n        for image in images:\n            feature = self.features[image][0]\n            \n            captions = batch.loc[batch[self.X_col]==image, self.y_col].tolist()\n            for caption in captions:\n                seq = self.tokenizer.texts_to_sequences([caption])[0]\n\n                for i in range(1,len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n                    X1.append(feature)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            \n        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n                \n        return X1, X2, y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Modelling**\n- The image embedding representations are concatenated with the first word of sentence ie. starseq and passed to the LSTM network \n- The LSTM network starts generating words after each input thus forming a sentence at the end","metadata":{}},{"cell_type":"markdown","source":"<img src='https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/03-advanced/image_captioning/png/model.png'>","metadata":{}},{"cell_type":"code","source":"## input1 = Input(shape=(1920,))\n##input2 = Input(shape=(max_length,))\n\n##img_features = Dense(256, activation='relu')(input1)\n##img_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n\n##sentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n##merged = concatenate([img_features_reshaped,sentence_features],axis=1)\n##sentence_features = LSTM(256)(merged)\n##x = Dropout(0.5)(sentence_features)\n##x = add([x, img_features])\n##x = Dense(128, activation='relu')(x)\n##x = Dropout(0.5)(x)\n##output = Dense(vocab_size, activation='softmax')(x)\n\n##caption_model = Model(inputs=[input1,input2], outputs=output)\n##caption_model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n\n\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, concatenate, add, Attention, RepeatVector, TimeDistributed, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\n# Define constants\nembedding_size = 256\nmax_length = 34\nvocab_size = 10000  # Change this to your actual vocabulary size\n\n# Define image model\nimage_input = Input(shape=(1920,))\nimg_features = Dense(256, activation='relu')(image_input)\nimg_features_repeated = RepeatVector(max_length)(img_features)\n\n# Define language model\nlanguage_input = Input(shape=(max_length,))\nsentence_features = Embedding(vocab_size, 256, mask_zero=True)(language_input)\nsentence_features = LSTM(128, return_sequences=True)(sentence_features)  # Reduced number of units\nsentence_features = TimeDistributed(Dense(256))(sentence_features)\n\n# Apply attention\nattention = Attention()([img_features_repeated, sentence_features])\nmerged = concatenate([img_features_repeated, attention], axis=-1)\n\n# Further processing with LSTM\nx = LSTM(128, return_sequences=True)(merged)  # Reduced number of units\nx = LSTM(256, return_sequences=False)(x)      # Reduced number of units\nx = Dropout(0.5)(x)\n\n# Ensure same dimensions before add\nimg_features_resized = Dense(256)(img_features)  # Adjust dimensions to match LSTM output\n\nx = add([x, img_features_resized])\nx = Dense(128, activation='relu')(x)          # Reduced number of units\nx = Dropout(0.5)(x)\noutput = Dense(vocab_size, activation='softmax')(x)\n\n# Model definition\ncaption_model = Model(inputs=[image_input, language_input], outputs=output)\ncaption_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Model Modification**\n- A slight change has been made in the original model architecture to push the performance. The image feature embeddings are added to the output of the LSTMs and then passed on to the fully connected layers\n- This slightly improves the performance of the model orignally proposed back in 2014: __Show and Tell: A Neural Image Caption Generator__ (https://arxiv.org/pdf/1411.4555.pdf)","metadata":{}},{"cell_type":"code","source":"plot_model(caption_model, to_file='caption_model.png', show_shapes=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain_generator = CustomDataGenerator(df=train,X_col='image',y_col='caption',batch_size=32,directory=image_path,\n                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)\n\nvalidation_generator = CustomDataGenerator(df=test,X_col='image',y_col='caption',batch_size=32,directory=image_path,\n                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"model.h5\"\ncheckpoint = ModelCheckpoint(model_name,\n                            monitor=\"val_loss\",\n                            mode=\"min\",\n                            save_best_only = True,\n                            verbose=1)\n\nearlystopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 10, verbose = 2, restore_best_weights=True)\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.2, \n                                            min_lr=0.00000001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Let's train the Model !**\n\n<img src='https://miro.medium.com/max/1400/1*xIXqf46yYonSXkUOWcOCvg.gif'>","metadata":{}},{"cell_type":"code","source":"history = caption_model.fit(\n        train_generator,\n        epochs=50,\n        validation_data=validation_generator,\n        callbacks=[checkpoint,earlystopping,learning_rate_reduction])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Inference**\n- Learning Curve (Loss Curve)\n- Assessment of Generated Captions (by checking the relevance of the caption with respect to the image, BLEU Score will not be used in this kernel)","metadata":{}},{"cell_type":"markdown","source":"## **Learning Curve**\n- The model has clearly overfit, possibly due to less amount of data\n- We can tackle this problem in two ways\n    1. Train the model on a larger dataset Flickr40k\n    2. Attention Models","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Caption Generation Utility Functions**\n- Utility functions to generate the captions of input images at the inference time.\n- Here the image embeddings are passed along with the first word, followed by which the text embedding of each new word is passed to generate the next word","metadata":{}},{"cell_type":"code","source":"def idx_to_word(integer,tokenizer):\n    \n    for word, index in tokenizer.word_index.items():\n        if index==integer:\n            return word\n    return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_caption(model, image, tokenizer, max_length, features, word2idx, idx2word):\n    \"\"\"\n    Generate a caption for an image using a trained model.\n    \n    Parameters:\n    - model: The trained captioning model.\n    - image: The image file name for which to generate the caption.\n    - tokenizer: The tokenizer used for encoding captions.\n    - max_length: The maximum length of the generated caption.\n    - features: A dictionary mapping image filenames to their extracted features.\n    - word2idx: A dictionary mapping words to their respective indices.\n    - idx2word: A dictionary mapping indices to their respective words.\n    \n    Returns:\n    - A string containing the generated caption.\n    \"\"\"\n    \n    # Extract the feature vector for the given image\n    feature = features[image]\n    \n    # Initialize the input text with the start token\n    in_text = \"startseq\"\n    \n    # Generate the caption word by word\n    for _ in range(max_length):\n        # Convert the current input text to a sequence of indices\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        \n        # Pad the sequence to the maximum length\n        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n        \n        # Predict the next word in the sequence\n        y_pred = model.predict([np.array([feature]), np.array(sequence)])\n        \n        # Get the index of the predicted word\n        y_pred = np.argmax(y_pred)\n        \n        # Convert the index to the corresponding word\n        word = idx2word.get(y_pred)\n        \n        # If the word is None, break the loop\n        if word is None:\n            break\n        \n        # Append the predicted word to the input text\n        in_text += \" \" + word\n        \n        # If the predicted word is the end token, break the loop\n        if word == 'endseq':\n            break\n            \n    # Return the generated caption, excluding the start and end tokens\n    return ' '.join(in_text.split()[1:-1])\n\n# Example usage:\n# caption = predict_caption(final_model, 'example_image.jpg', tokenizer, max_length, features, word2idx, idx2word)\n# print(caption)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Taking 15 Random Samples for Caption Prediction**","metadata":{}},{"cell_type":"code","source":"samples = test.sample(15)\nsamples.reset_index(drop=True,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index,record in samples.iterrows():\n\n    img = load_img(os.path.join(image_path,record['image']),target_size=(224,224))\n    img = img_to_array(img)\n    img = img/255.\n    \n    caption = predict_caption(caption_model,record['image'], tokenizer, max_length, features, word2idx, idx2word)\n    samples.loc[index,'caption'] = caption","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Results**\n- As we can clearly see there is some redundant caption generation e.g. Dog running through the water, overusage of blue shirt for any other coloured cloth\n- The model performance can be further improved by training on more data and using attention mechanism so that our model can focus on relevant areas during the text generation\n- We can also leverage the interprettability of the attention mechanism to understand which areas of the image leads to the generation of which word","metadata":{}},{"cell_type":"code","source":"display_images(samples)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='font-size: 18px'><strong>Conclusion: </strong>This may not be the best performing model, but the objective of this kernel is to give a gist of how Image Captioning problems can be approached. In the future work of this kernel <strong>Attention model</strong> training and <strong>BLEU Score</strong> assessment will be performed.</p>","metadata":{}},{"cell_type":"code","source":"!pip install streamlit -q","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:06:15.349616Z","iopub.execute_input":"2024-05-21T11:06:15.350456Z","iopub.status.idle":"2024-05-21T11:06:30.914810Z","shell.execute_reply.started":"2024-05-21T11:06:15.350413Z","shell.execute_reply":"2024-05-21T11:06:30.913691Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom PIL import Image\nfrom sklearn.cluster import KMeans\nimport seaborn as sns\nfrom tensorflow.keras.initializers import Orthogonal","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:06:30.916708Z","iopub.execute_input":"2024-05-21T11:06:30.917051Z","iopub.status.idle":"2024-05-21T11:06:31.028194Z","shell.execute_reply.started":"2024-05-21T11:06:30.917018Z","shell.execute_reply":"2024-05-21T11:06:31.027418Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model.summary","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download ngrok\n!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n\n# Unzip the downloaded file\n!unzip ngrok-stable-linux-amd64.zip","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:06:31.029148Z","iopub.execute_input":"2024-05-21T11:06:31.029454Z","iopub.status.idle":"2024-05-21T11:06:34.628741Z","shell.execute_reply.started":"2024-05-21T11:06:31.029426Z","shell.execute_reply":"2024-05-21T11:06:34.627491Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"--2024-05-21 11:06:31--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\nResolving bin.equinox.io (bin.equinox.io)... 54.161.241.46, 18.205.222.128, 54.237.133.81, ...\nConnecting to bin.equinox.io (bin.equinox.io)|54.161.241.46|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13921656 (13M) [application/octet-stream]\nSaving to: ‘ngrok-stable-linux-amd64.zip’\n\nngrok-stable-linux- 100%[===================>]  13.28M  13.9MB/s    in 1.0s    \n\n2024-05-21 11:06:33 (13.9 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n\nArchive:  ngrok-stable-linux-amd64.zip\n  inflating: ngrok                   \n","output_type":"stream"}]},{"cell_type":"code","source":"!chmod +x ngrok","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:06:34.631062Z","iopub.execute_input":"2024-05-21T11:06:34.631506Z","iopub.status.idle":"2024-05-21T11:06:35.590782Z","shell.execute_reply.started":"2024-05-21T11:06:34.631465Z","shell.execute_reply":"2024-05-21T11:06:35.589746Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"!./ngrok authtokens 2gTgesqVJ5fXr6FiY4N7R46L7hs_5FKChinrwKGCVpZw34uEA","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:06:35.592353Z","iopub.execute_input":"2024-05-21T11:06:35.592781Z","iopub.status.idle":"2024-05-21T11:06:36.560409Z","shell.execute_reply.started":"2024-05-21T11:06:35.592744Z","shell.execute_reply":"2024-05-21T11:06:36.559335Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"NAME:\n   ngrok - tunnel local ports to public URLs and inspect traffic\n\nDESCRIPTION:\n    ngrok exposes local networked services behinds NATs and firewalls to the\n    public internet over a secure tunnel. Share local websites, build/test\n    webhook consumers and self-host personal services.\n    Detailed help for each command is available with 'ngrok help <command>'.\n    Open http://localhost:4040 for ngrok's web interface to inspect traffic.\n\nEXAMPLES:\n    ngrok http 80                    # secure public URL for port 80 web server\n    ngrok http -subdomain=baz 8080   # port 8080 available at baz.ngrok.io\n    ngrok http foo.dev:80            # tunnel to host:port instead of localhost\n    ngrok http https://localhost     # expose a local https server\n    ngrok tcp 22                     # tunnel arbitrary TCP traffic to port 22\n    ngrok tls -hostname=foo.com 443  # TLS traffic for foo.com to port 443\n    ngrok start foo bar baz          # start tunnels from the configuration file\n\nVERSION:\n   2.3.41\n\nAUTHOR:\n  inconshreveable - <alan@ngrok.com>\n\nCOMMANDS:\n   authtoken\tsave authtoken to configuration file\n   credits\tprints author and licensing information\n   http\t\tstart an HTTP tunnel\n   start\tstart tunnels by name from the configuration file\n   tcp\t\tstart a TCP tunnel\n   tls\t\tstart a TLS tunnel\n   update\tupdate ngrok to the latest version\n   version\tprint the version string\n   help\t\tShows a list of commands or help for one command\n\nERROR:  Unrecognized command: authtokens\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile app.py\nimport streamlit as st\nst.title('Image Captioning App')","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:06:36.562191Z","iopub.execute_input":"2024-05-21T11:06:36.562545Z","iopub.status.idle":"2024-05-21T11:06:36.569104Z","shell.execute_reply.started":"2024-05-21T11:06:36.562512Z","shell.execute_reply":"2024-05-21T11:06:36.568226Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Writing app.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile app.py\nimport streamlit as st\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.applications import DenseNet201\nfrom tensorflow.keras.applications.densenet import preprocess_input\nimport pickle\nfrom PIL import Image\nfrom io import BytesIO\n\nmax_length= 34\n# Load your trained captioning model\ncaption_model = load_model('/kaggle/input/image_model/tensorflow2/model1/1/model (1).h5')\n\nwith open('/kaggle/working/tokenizer.pkl', 'rb') as handle:\n    tokenizer = pickle.load(handle)\n    \ndef idx_to_word(integer,tokenizer):\n    \n    for word, index in tokenizer.word_index.items():\n        if index==integer:\n            return word\n    return None\n    \nmodel = DenseNet201(weights='imagenet', include_top=False, pooling='avg')\nfe = Model(inputs=model.input, outputs=model.layers[-2].output)\n\n\nfrom keras.layers import GlobalAveragePooling2D\n\ndef predict_caption(model, image_file, tokenizer, max_length):\n    # Convert the file to bytes and open it as an image\n    img = Image.open(BytesIO(image_file.getvalue()))\n\n    # Convert the image to an array and preprocess it\n    img = img_to_array(img)\n    img = preprocess_input(img)\n\n    # Reshape the image to match the input shape of the DenseNet model\n    img = np.expand_dims(img, axis=0)\n\n    # Extract features using DenseNet201\n    feature = fe.predict(img, verbose=0)\n\n    # Apply global average pooling to the feature map\n    gap = GlobalAveragePooling2D()(feature)\n\n    in_text = \"startseq\"\n    for i in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], max_length)\n\n        y_pred = model.predict([gap,sequence])\n        y_pred = np.argmax(y_pred)\n        \n        word = idx_to_word(y_pred, tokenizer)\n        \n        if word is None:\n            break\n            \n        in_text+= \" \" + word\n        \n        if word == 'endseq':\n            break\n        \n        final_caption = in_text.replace('startseq', '').replace('endseq', '').strip()\n    return final_caption \n\n# Streamlit app\nst.title('Image Captioning App')\n\nuploaded_file = st.file_uploader('Choose an image...', type='jpg')\n\nif uploaded_file is not None:\n    # Generate the caption\n    caption = predict_caption(caption_model, uploaded_file, tokenizer, max_length)\n    \n    # Display the image and the caption\n    st.image(uploaded_file, caption='Uploaded Image', use_column_width=True)\n    st.write('Generated Caption: ', caption)","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:12:53.391953Z","iopub.execute_input":"2024-05-21T11:12:53.392764Z","iopub.status.idle":"2024-05-21T11:12:53.401105Z","shell.execute_reply.started":"2024-05-21T11:12:53.392718Z","shell.execute_reply":"2024-05-21T11:12:53.400268Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Overwriting app.py\n","output_type":"stream"}]},{"cell_type":"code","source":"!npm install localtunnel","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:06:45.540211Z","iopub.execute_input":"2024-05-21T11:06:45.541136Z","iopub.status.idle":"2024-05-21T11:06:50.327339Z","shell.execute_reply.started":"2024-05-21T11:06:45.541068Z","shell.execute_reply":"2024-05-21T11:06:50.326300Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/kaggle/working/package.json'\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/kaggle/working/package.json'\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m working No description\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m working No repository field.\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m working No README data\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m working No license field.\n\u001b[0m\n+ localtunnel@2.0.2\nadded 22 packages from 22 contributors and audited 22 packages in 1.542s\n\n3 packages are looking for funding\n  run `npm fund` for details\n\nfound 1 \u001b[93mmoderate\u001b[0m severity vulnerability\n  run `npm audit fix` to fix them, or `npm audit` for details\n\u001b[K\u001b[?25h7m            \u001b[27m\u001b[90m......\u001b[0m] | postinstall: \u001b[7msill\u001b[0m \u001b[35minstall\u001b[0m printInstalled\u001b[0m\u001b[K","output_type":"stream"}]},{"cell_type":"code","source":"!streamlit run app.py & npx localtunnel --port 8501","metadata":{"execution":{"iopub.status.busy":"2024-05-21T11:12:56.799607Z","iopub.execute_input":"2024-05-21T11:12:56.800508Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\nCollecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n\u001b[0m\n\u001b[0m\n\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n\u001b[0m\n\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.19.2.2:8501\u001b[0m\n\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.204.119.156:8501\u001b[0m\n\u001b[0m\n\u001b[K\u001b[?25h####......] - refresh-package-json:localtunnel: timing action:finalize\u001b[0m\u001b[K[0m\u001b[Knpx: installed 22 in 4.096s\nyour url is: https://dull-streets-enjoy.loca.lt\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}